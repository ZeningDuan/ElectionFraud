{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapinghub in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (from scrapinghub) (1.15.0)\n",
      "Requirement already satisfied: requests>=1.0 in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (from scrapinghub) (2.23.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (from scrapinghub) (1.3.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (from requests>=1.0->scrapinghub) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (from requests>=1.0->scrapinghub) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (from requests>=1.0->scrapinghub) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (from requests>=1.0->scrapinghub) (2020.6.20)\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapinghub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapinghub-autoextract in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (0.6.1)\n",
      "Requirement already satisfied: attrs in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (from scrapinghub-autoextract) (20.3.0)\n",
      "Requirement already satisfied: requests in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (from scrapinghub-autoextract) (2.23.0)\n",
      "Requirement already satisfied: tqdm; python_version >= \"3.6\" in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (from scrapinghub-autoextract) (4.46.0)\n",
      "Requirement already satisfied: runstats in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (from scrapinghub-autoextract) (1.8.0)\n",
      "Requirement already satisfied: tenacity; python_version >= \"3.6\" in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (from scrapinghub-autoextract) (7.0.0)\n",
      "Requirement already satisfied: aiohttp>=3.6.0; python_version >= \"3.6\" in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (from scrapinghub-autoextract) (3.7.4.post0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (from requests->scrapinghub-autoextract) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (from requests->scrapinghub-autoextract) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (from requests->scrapinghub-autoextract) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (from requests->scrapinghub-autoextract) (2020.6.20)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (from tenacity; python_version >= \"3.6\"->scrapinghub-autoextract) (1.15.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (from aiohttp>=3.6.0; python_version >= \"3.6\"->scrapinghub-autoextract) (5.1.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (from aiohttp>=3.6.0; python_version >= \"3.6\"->scrapinghub-autoextract) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (from aiohttp>=3.6.0; python_version >= \"3.6\"->scrapinghub-autoextract) (3.10.0.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ze/opt/miniconda3/lib/python3.7/site-packages (from aiohttp>=3.6.0; python_version >= \"3.6\"->scrapinghub-autoextract) (1.6.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapinghub-autoextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autoextract'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bc9d15c43062>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mautoextract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequest_raw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autoextract'"
     ]
    }
   ],
   "source": [
    "from autoextract.sync import request_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scrapinghub-autoextract\n",
    "#!pip install auto-extract\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "#from autoextract.sync import request_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If Only A Few URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "query = [{\n",
    "    'url': 'http://example.com/article?id=24',\n",
    "    'pageType': 'article'\n",
    "}]\n",
    "results = request_raw(query, api_key='[api key]')\n",
    "print(results[0]['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'query': {'id': '1620506668704-0d5a3d545efcfdbc', 'domain': 'msn.com', 'userAgent': 'python-requests/2.24.0', 'userQuery': {'url': 'https://www.msn.com/en-us/news/world/south-korea-has-reported-additional-48-cases-of-a-new-virus-raising-its-total-to-204/ar-BB10eeyv', 'pageType': 'article'}}, 'error': 'Downloader error: http404'}]\n"
     ]
    }
   ],
   "source": [
    "#import requests\n",
    "\n",
    "response = requests.post(\n",
    "    'https://autoextract.scrapinghub.com/v1/extract',\n",
    "    auth=('531660b7c27448f1b355c16abd639291', ''),\n",
    "    json=[{'url': 'https://www.msn.com/en-us/news/world/south-korea-has-reported-additional-48-cases-of-a-new-virus-raising-its-total-to-204/ar-BB10eeyv', 'pageType': 'article'}],\n",
    "    timeout=605,\n",
    ")\n",
    "\n",
    "a = response.json()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query', 'error'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IF AT SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractNews(urls):\n",
    "    Headline = []\n",
    "    ArticleBody = []\n",
    "    Authors = []\n",
    "    DatePublished = []\n",
    "    DateModified = []\n",
    "    Domain = []\n",
    "    NewsUrls = []\n",
    "    ImageUrls = []\n",
    "    VideoUrls = []\n",
    "    AudioUrls = []\n",
    "    Prop = []\n",
    "    MissingIndexes = []\n",
    "    \n",
    "    a = 0\n",
    "    \n",
    "    for i in range(len(urls)):\n",
    "        a += 1\n",
    "        if a % 100 == 0:\n",
    "            print(a)\n",
    "        \n",
    "        try:\n",
    "            response = requests.post('https://autoextract.scrapinghub.com/v1/extract',\n",
    "                                    auth=('531660b7c27448f1b355c16abd639291', ''),\n",
    "                                    json=[{'url': urls[i], 'pageType': 'article'}],\n",
    "                                    timeout=605\n",
    "                                  )\n",
    "            \n",
    "            #pls check official document: https://doc.scrapinghub.com/autoextract/article.html\n",
    "            #get headline\n",
    "            try:\n",
    "                Headline.append(response.json()[0]['article'].get('headline'))\n",
    "            except:\n",
    "                Headline.append('Error')\n",
    "            #get articlebody     \n",
    "            try:\n",
    "                ArticleBody.append(response.json()[0]['article'].get('articleBody')) \n",
    "            except:\n",
    "                ArticleBody.append(\"Error\")\n",
    "            #get authors\n",
    "            try:\n",
    "                Authors.append(response.json()[0]['article'].get('authorsList'))\n",
    "            except:\n",
    "                Authors.append('Error')\n",
    "            #get published date\n",
    "            try:\n",
    "                DatePublished.append(response.json()[0]['article'].get('datePublished'))\n",
    "            except:\n",
    "                DatePublished.append('Error')\n",
    "            #get modified date\n",
    "            try:\n",
    "                DateModified.append(response.json()[0]['article'].get('dateModified'))\n",
    "            except:\n",
    "                DateModified.append('Error')\n",
    "            #get domain\n",
    "            try:\n",
    "                Domain.append(response.json()[0]['query'].get('domain'))\n",
    "            except:\n",
    "                Domain.append('Error')\n",
    "            #get url\n",
    "            try:\n",
    "                NewsUrls.append(response.json()[0]['article'].get('url'))\n",
    "            except:\n",
    "                NewsUrls.append('Error')\n",
    "            #get image url\n",
    "            try:\n",
    "                ImageUrls.append(response.json()[0]['article'].get('images'))\n",
    "            except:\n",
    "                ImageUrls.append('Error')  \n",
    "            #get video url\n",
    "            try:\n",
    "                VideoUrls.append(response.json()[0]['article'].get('videoUrls'))\n",
    "            except:\n",
    "                VideoUrls.append('Error') \n",
    "            #get audio url\n",
    "            try:\n",
    "                AudioUrls.append(response.json()[0]['article'].get('audioUrls'))\n",
    "            except:\n",
    "                AudioUrls.append('Error') \n",
    "            #get Propobility of being a single page article\n",
    "            try:\n",
    "                Prop.append(response.json()[0]['article'].get('probability'))\n",
    "            except:\n",
    "                Prop.append('Error') \n",
    "            \n",
    "        \n",
    "        except: \n",
    "            MissingIndexes.append(urls[i])\n",
    "            \n",
    "        \n",
    "            \n",
    "        df = pd.DataFrame({\n",
    "            'Head line' : Headline,\n",
    "            'ArticleBody' : ArticleBody,\n",
    "            'Authors' : Authors,\n",
    "            'DatePublished' : DatePublished,\n",
    "            'DateModified' : DateModified,\n",
    "            'Domain' : Domain,\n",
    "            'NewsUrls' : NewsUrls,\n",
    "            'ImageUrls' : ImageUrls,\n",
    "            'VideoUrls' : VideoUrls,\n",
    "            'AudioUrls' : AudioUrls,\n",
    "            'Prop' : Prop\n",
    "        }\n",
    "        )\n",
    "             \n",
    "    return df, MissingIndexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run function\n",
    "newfilename_list = []\n",
    "path = '/Volumes/ZeDrive1/Projects/Covid_Bot Agenda setting/News Data/COVID-19_Media Cloud/Foxnews_2020/'\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for filename in files:\n",
    "        if \"Foxnews\" in filename and \"._\" not in filename:\n",
    "            tic = time.time()\n",
    "            data = pd.read_csv(path + filename)\n",
    "            urls = list(data['url'])\n",
    "            results = ExtractNews(urls)[0]\n",
    "         \n",
    "            newfilename = filename.replace('Foxnews','Foxnews_data')\n",
    "            newfilename_list.append(newfilename)\n",
    "            \n",
    "            results.to_csv(path + newfilename)\n",
    "            toc = time.time()\n",
    "            print(\"Now is \", time.ctime(toc), \", we spent \", round(toc - tic, 2),  \" for scraping \", filename, \" data.\")\n",
    "            print(\"In total, there have \", str(len(data)), \"URLs. We obtained \", str(len(results)), \"news data.\")\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
